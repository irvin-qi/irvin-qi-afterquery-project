# LLM Analysis Feature Plan

## Overview

Add an LLM-powered code analysis feature that takes the review rubric and file diffs, then generates a summary of the code and how well it adheres to the rubric. This will appear as a new tab in the review workspace alongside existing tabs: Summary, Scoring, Diff guidance, and Feedback.

## Goals

1. **Automated Code Analysis**: Use LLM to analyze candidate code against the review rubric
2. **Rubric Adherence Evaluation**: Assess how well the code meets each rubric criterion
3. **Summary Generation**: Provide a concise summary of code quality and adherence
4. **Caching**: Store analyses to avoid redundant LLM calls
5. **User Experience**: Seamless integration with existing review workflow
6. **Interactive Q&A**: Allow reviewers to ask additional questions about the codebase with full context (rubric + diffs)
7. **Rubric Context**: Always include the full review rubric as context in all LLM interactions

## Architecture

### Components

1. **Frontend**:
   - New tab component in review page
   - Loading states and error handling
   - Display formatted LLM analysis
   - Interactive chat interface for Q&A

2. **Backend**:
   - New API endpoint for generating/retrieving analysis
   - API endpoint for asking questions (with conversation history)
   - LLM service integration module
   - Database models for caching analyses and conversation history

3. **LLM Service**:
   - Provider abstraction (OpenAI, Anthropic, etc.)
   - Prompt engineering for code review context
   - Always includes rubric as context
   - Token management and rate limiting
   - Conversation history management

## Database Schema

### New Table: `review_llm_analyses`

```sql
CREATE TABLE IF NOT EXISTS review_llm_analyses (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  invitation_id uuid REFERENCES invitations(id) ON DELETE CASCADE NOT NULL,
  analysis_text text NOT NULL,
  raw_response jsonb, -- Store full LLM response for debugging/auditing
  model_used text, -- e.g., "gpt-4", "claude-3-opus"
  prompt_version text, -- Track prompt template version
  created_by uuid REFERENCES users(id),
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now(),
  UNIQUE (invitation_id) -- One analysis per invitation (can be regenerated by deleting)
);

CREATE INDEX IF NOT EXISTS idx_review_llm_analyses_invitation_id 
  ON review_llm_analyses(invitation_id);
CREATE INDEX IF NOT EXISTS idx_review_llm_analyses_created_at 
  ON review_llm_analyses(created_at DESC);
```

**Rationale**:
- Store analysis to avoid redundant API calls
- Track which model/prompt was used for auditability
- Single analysis per invitation (can be regenerated by deleting the record)
- Store raw response for debugging and future analysis

### New Table: `review_llm_conversations`

```sql
CREATE TABLE IF NOT EXISTS review_llm_conversations (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  invitation_id uuid REFERENCES invitations(id) ON DELETE CASCADE NOT NULL,
  message_type text CHECK (message_type IN ('user', 'assistant')) NOT NULL,
  message_text text NOT NULL,
  context_snapshot jsonb, -- Store rubric + diff summary at time of message
  model_used text,
  created_by uuid REFERENCES users(id),
  created_at timestamptz DEFAULT now()
);

CREATE INDEX IF NOT EXISTS idx_review_llm_conversations_invitation_id 
  ON review_llm_conversations(invitation_id, created_at);
```

**Rationale**:
- Store conversation history for context in follow-up questions
- Track message type (user question vs assistant response)
- Store context snapshot (rubric + diff summary) with each message for reproducibility
- Allows reviewers to ask follow-up questions with full conversation context

## Backend Implementation

### 1. Database Model (`backend/app/models.py`)

```python
class ReviewLLMAnalysis(Base, TimestampMixin):
    __tablename__ = "review_llm_analyses"
    __table_args__ = (
        Index("idx_review_llm_analyses_invitation_id", "invitation_id"),
        Index("idx_review_llm_analyses_created_at", "created_at"),
    )

    id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True), primary_key=True, default=uuid.uuid4
    )
    invitation_id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True),
        ForeignKey("invitations.id", ondelete="CASCADE"),
        nullable=False,
        unique=True,  # One analysis per invitation
    )
    analysis_text: Mapped[str] = mapped_column(Text, nullable=False)
    raw_response: Mapped[Optional[dict]] = mapped_column(JSONB, nullable=True)
    model_used: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    prompt_version: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    created_by: Mapped[Optional[uuid.UUID]] = mapped_column(
        UUID(as_uuid=True), ForeignKey("users.id"), nullable=True
    )

    invitation: Mapped[Invitation] = relationship(back_populates="llm_analyses")
```

### 2. Schema (`backend/app/schemas.py`)

```python
class ReviewLLMAnalysisCreate(BaseModel):
    invitation_id: UUID
    regenerate: bool = False  # If True, delete existing and regenerate

class ReviewLLMAnalysisRead(CamelModel):
    id: str
    invitation_id: str
    analysis_text: str
    model_used: Optional[str] = None
    prompt_version: Optional[str] = None
    created_at: datetime
    created_by: Optional[str] = None

class LLMQuestionCreate(BaseModel):
    question: str  # User's question about the codebase

class LLMConversationMessageRead(CamelModel):
    id: str
    invitation_id: str
    message_type: str  # "user" or "assistant"
    message_text: str
    model_used: Optional[str] = None
    created_at: datetime
    created_by: Optional[str] = None
```

### 3. LLM Service Module (`backend/app/services/llm_service.py`)

Create a new service module to handle LLM interactions:

```python
# backend/app/services/llm_service.py

from abc import ABC, abstractmethod
from typing import Optional
import os

class LLMProvider(ABC):
    @abstractmethod
    async def generate_analysis(
        self,
        rubric: str,
        diff_text: str,
        file_summary: str,
    ) -> dict:
        """Generate analysis from rubric and diffs. Returns dict with 'text' and 'model' keys.
        
        NOTE: rubric MUST always be included as primary context.
        """
        pass
    
    @abstractmethod
    async def answer_question(
        self,
        rubric: str,
        diff_text: str,
        file_summary: str,
        question: str,
        conversation_history: Optional[list[dict]] = None,
        initial_analysis: Optional[str] = None,
    ) -> dict:
        """Answer a question about the codebase with full context.
        
        Args:
            rubric: The review rubric (ALWAYS included as primary context)
            diff_text: Full diff text
            file_summary: Summary of changed files
            question: User's question
            conversation_history: Previous messages in format [{"role": "user"/"assistant", "content": "..."}]
            initial_analysis: The initial LLM analysis if it exists
        
        Returns dict with 'text' and 'model' keys.
        """
        pass

class OpenAIProvider(LLMProvider):
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4"):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not set")

    async def generate_analysis(
        self,
        rubric: str,
        diff_text: str,
        file_summary: str,
    ) -> dict:
        # Implementation using OpenAI API
        # Use httpx for async requests
        pass

class AnthropicProvider(LLMProvider):
    def __init__(self, api_key: Optional[str] = None, model: str = "claude-3-opus-20240229"):
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        self.model = model
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not set")

    async def generate_analysis(
        self,
        rubric: str,
        diff_text: str,
        file_summary: str,
    ) -> dict:
        # Implementation using Anthropic API
        pass

def get_llm_provider() -> LLMProvider:
    """Factory function to get the configured LLM provider."""
    provider = os.getenv("LLM_PROVIDER", "openai").lower()
    
    if provider == "openai":
        return OpenAIProvider(model=os.getenv("OPENAI_MODEL", "gpt-4"))
    elif provider == "anthropic":
        return AnthropicProvider(model=os.getenv("ANTHROPIC_MODEL", "claude-3-opus-20240229"))
    else:
        raise ValueError(f"Unknown LLM provider: {provider}")
```

### 4. Prompt Templates

#### 4.1 Initial Analysis Prompt

**CRITICAL: The rubric MUST always be included as primary context in all LLM interactions.**

The prompt should be structured to:
- Provide context about code review
- **Always include the full review rubric as the primary evaluation criteria**
- Include relevant file diffs
- Request structured output (summary + rubric adherence)

Example prompt template:

```
You are an expert code reviewer analyzing a candidate's submission for a coding assessment.

## Review Rubric
The following rubric defines the criteria for evaluating this submission. Use this rubric as the primary reference for all your assessments:

{rubric_text}

## Code Changes
The candidate has made the following changes to the codebase:

### File Summary
{file_summary}

### Detailed Diffs
{diff_text}

## Your Task
Please provide:
1. A concise summary of what the code does and its overall quality
2. An evaluation of how well the code adheres to EACH criterion in the rubric above
3. Specific strengths and areas for improvement, referencing the rubric criteria
4. A final assessment score (1-5) if appropriate

Format your response in clear, structured markdown.
```

#### 4.2 Q&A Conversation Prompt

For interactive questions, always include the rubric and conversation history:

```
You are an expert code reviewer helping evaluate a candidate's submission for a coding assessment.

## Review Rubric
Use this rubric as the primary reference for all evaluations:

{rubric_text}

## Code Changes Summary
The candidate has made the following changes to the codebase:

### File Summary
{file_summary}

### Detailed Diffs
{diff_text}

## Previous Analysis
{initial_analysis_text}

## Conversation History
{conversation_history}

## Current Question
{user_question}

Please answer the question with reference to:
- The review rubric above
- The code changes shown
- Any relevant context from the previous analysis
- Be specific and cite examples from the code when possible
```

### 5. API Endpoints (`backend/app/routes/reviews.py`)

Add new endpoints:

```python
@router.get("/invitations/{invitation_id}/llm-analysis", response_model=schemas.ReviewLLMAnalysisRead)
async def get_llm_analysis(
    invitation_id: str,
    session: AsyncSession = Depends(get_session),
    current_session: SupabaseSession = Depends(require_roles("authenticated", "service_role")),
) -> schemas.ReviewLLMAnalysisRead:
    """Get existing LLM analysis for an invitation."""
    # Verify access, fetch from DB, return
    pass

@router.post("/invitations/{invitation_id}/llm-analysis/generate", response_model=schemas.ReviewLLMAnalysisRead)
async def generate_llm_analysis(
    invitation_id: str,
    payload: schemas.ReviewLLMAnalysisCreate,
    session: AsyncSession = Depends(get_session),
    current_session: SupabaseSession = Depends(require_roles("authenticated", "service_role")),
) -> schemas.ReviewLLMAnalysisRead:
    """
    Generate or regenerate LLM analysis for an invitation.
    
    Steps:
    1. Verify invitation access
    2. Get assessment rubric (REQUIRED - must exist)
    3. Get repo diff data
    4. Format prompt with rubric + diffs (rubric is primary context)
    5. Call LLM service
    6. Store result in DB
    7. Return analysis
    """
    pass

@router.get("/invitations/{invitation_id}/llm-conversation", response_model=list[schemas.LLMConversationMessageRead])
async def get_conversation_history(
    invitation_id: str,
    session: AsyncSession = Depends(get_session),
    current_session: SupabaseSession = Depends(require_roles("authenticated", "service_role")),
) -> list[schemas.LLMConversationMessageRead]:
    """Get conversation history for an invitation."""
    # Verify access, fetch messages ordered by created_at, return
    pass

@router.post("/invitations/{invitation_id}/llm-conversation/ask", response_model=schemas.LLMConversationMessageRead)
async def ask_question(
    invitation_id: str,
    payload: schemas.LLMQuestionCreate,
    session: AsyncSession = Depends(get_session),
    current_session: SupabaseSession = Depends(require_roles("authenticated", "service_role")),
) -> schemas.LLMConversationMessageRead:
    """
    Ask a question about the codebase. Includes full context:
    - Review rubric (ALWAYS included)
    - Code diffs
    - Initial analysis (if exists)
    - Conversation history
    
    Steps:
    1. Verify invitation access
    2. Get assessment rubric (REQUIRED)
    3. Get repo diff data
    4. Get initial analysis (if exists)
    5. Get conversation history
    6. Build prompt with ALL context (rubric is primary)
    7. Call LLM provider
    8. Store both user question and assistant response in DB
    9. Return assistant response
    """
    pass
```

**Implementation flow for generate_llm_analysis**:
1. Verify user has access to invitation
2. Fetch assessment to get `rubric_text` (REQUIRED - return error if missing)
3. Fetch repo diff using existing diff endpoint logic
4. Format diffs into readable text (truncate if too long)
5. Build prompt with **rubric as primary context** + diffs
6. Call LLM provider
7. Store result in database
8. Return formatted response

**Implementation flow for ask_question**:
1. Verify user has access to invitation
2. Fetch assessment to get `rubric_text` (REQUIRED)
3. Fetch repo diff data
4. Fetch initial analysis (if exists)
5. Fetch conversation history (ordered by created_at)
6. Store user question in conversation table
7. Build prompt with rubric + diffs + analysis + conversation history
8. Call LLM provider with question
9. Store assistant response in conversation table
10. Return assistant response

**Error handling**:
- Handle missing rubric
- Handle missing/inaccessible repo
- Handle LLM API failures
- Handle token limit exceeded (truncate diffs)
- Handle invalid responses

### 6. Dependencies (`backend/requirements.txt`)

Add:
```
openai>=1.0.0  # If using OpenAI
anthropic>=0.7.0  # If using Anthropic
```

## Frontend Implementation

### 1. API Client Functions (`frontend/lib/api.ts`)

```typescript
export type ReviewLLMAnalysis = {
  id: string;
  invitationId: string;
  analysisText: string;
  modelUsed?: string | null;
  promptVersion?: string | null;
  createdAt: string;
  createdBy?: string | null;
};

export type LLMConversationMessage = {
  id: string;
  invitationId: string;
  messageType: "user" | "assistant";
  messageText: string;
  modelUsed?: string | null;
  createdAt: string;
  createdBy?: string | null;
};

export async function getLLMAnalysis(
  invitationId: string,
  options: ApiRequestOptions = {},
): Promise<ReviewLLMAnalysis> {
  return fetchJson<ReviewLLMAnalysis>(
    `/api/candidate-repos/invitations/${encodeURIComponent(invitationId)}/llm-analysis`,
    options,
  );
}

export async function generateLLMAnalysis(
  invitationId: string,
  regenerate: boolean = false,
  options: ApiRequestOptions = {},
): Promise<ReviewLLMAnalysis> {
  return fetchJson<ReviewLLMAnalysis>(
    `/api/candidate-repos/invitations/${encodeURIComponent(invitationId)}/llm-analysis/generate`,
    {
      ...options,
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(options.headers ?? {}),
      },
      body: JSON.stringify({ invitation_id: invitationId, regenerate }),
    },
  );
}

export async function getConversationHistory(
  invitationId: string,
  options: ApiRequestOptions = {},
): Promise<LLMConversationMessage[]> {
  return fetchJson<LLMConversationMessage[]>(
    `/api/candidate-repos/invitations/${encodeURIComponent(invitationId)}/llm-conversation`,
    options,
  );
}

export async function askQuestion(
  invitationId: string,
  question: string,
  options: ApiRequestOptions = {},
): Promise<LLMConversationMessage> {
  return fetchJson<LLMConversationMessage>(
    `/api/candidate-repos/invitations/${encodeURIComponent(invitationId)}/llm-conversation/ask`,
    {
      ...options,
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(options.headers ?? {}),
      },
      body: JSON.stringify({ question }),
    },
  );
}
```

### 2. Review Page Tab (`frontend/app/app/(admin)/review/[invitationId]/page.tsx`)

Add new tab:

```tsx
<Tabs defaultValue="summary">
  <TabsList>
    <TabsTrigger value="summary">Summary</TabsTrigger>
    <TabsTrigger value="scoring">Scoring</TabsTrigger>
    <TabsTrigger value="diff">Diff guidance</TabsTrigger>
    <TabsTrigger value="feedback">Feedback</TabsTrigger>
    <TabsTrigger value="llm-analysis">LLM Analysis</TabsTrigger>
  </TabsList>
  {/* ... existing tabs ... */}
  <TabsContent value="llm-analysis">
    <LLMAnalysisTab invitationId={invitation.id} accessToken={accessToken} />
  </TabsContent>
</Tabs>
```

### 3. LLM Analysis Component (`frontend/components/review/llm-analysis-tab.tsx`)

Create new component with both analysis and Q&A sections:

```tsx
"use client";

import { useState, useEffect, useRef } from "react";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@/components/ui/card";
import { Button } from "@/components/ui/button";
import { Markdown } from "@/components/ui/markdown";
import { Textarea } from "@/components/ui/textarea";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import { 
  getLLMAnalysis, 
  generateLLMAnalysis, 
  getConversationHistory,
  askQuestion,
  type ReviewLLMAnalysis,
  type LLMConversationMessage,
} from "@/lib/api";
import { Loader2, RefreshCw, Send, User, Bot } from "lucide-react";

export function LLMAnalysisTab({ invitationId, accessToken }: { invitationId: string; accessToken: string | null }) {
  const [analysis, setAnalysis] = useState<ReviewLLMAnalysis | null>(null);
  const [loading, setLoading] = useState(true);
  const [generating, setGenerating] = useState(false);
  const [error, setError] = useState<string | null>(null);
  
  // Conversation state
  const [conversation, setConversation] = useState<LLMConversationMessage[]>([]);
  const [loadingConversation, setLoadingConversation] = useState(true);
  const [question, setQuestion] = useState("");
  const [asking, setAsking] = useState(false);
  const conversationEndRef = useRef<HTMLDivElement>(null);

  useEffect(() => {
    loadAnalysis();
    loadConversation();
  }, [invitationId, accessToken]);

  useEffect(() => {
    // Scroll to bottom when conversation updates
    conversationEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [conversation]);

  async function loadAnalysis() {
    if (!accessToken) {
      setLoading(false);
      return;
    }

    setLoading(true);
    setError(null);
    try {
      const data = await getLLMAnalysis(invitationId, { accessToken });
      setAnalysis(data);
    } catch (err) {
      if (err instanceof Error && err.message.includes("404")) {
        setAnalysis(null);
      } else {
        setError(err instanceof Error ? err.message : "Failed to load analysis");
      }
    } finally {
      setLoading(false);
    }
  }

  async function loadConversation() {
    if (!accessToken) {
      setLoadingConversation(false);
      return;
    }

    setLoadingConversation(true);
    try {
      const messages = await getConversationHistory(invitationId, { accessToken });
      setConversation(messages);
    } catch (err) {
      console.error("Failed to load conversation:", err);
    } finally {
      setLoadingConversation(false);
    }
  }

  async function handleGenerate(regenerate: boolean = false) {
    if (!accessToken) {
      setError("Sign in to generate analysis");
      return;
    }

    setGenerating(true);
    setError(null);
    try {
      const data = await generateLLMAnalysis(invitationId, regenerate, { accessToken });
      setAnalysis(data);
    } catch (err) {
      setError(err instanceof Error ? err.message : "Failed to generate analysis");
    } finally {
      setGenerating(false);
    }
  }

  async function handleAskQuestion() {
    if (!accessToken || !question.trim() || asking) {
      return;
    }

    const userQuestion = question.trim();
    setQuestion("");
    setAsking(true);
    setError(null);

    try {
      // Optimistically add user message
      const userMessage: LLMConversationMessage = {
        id: `temp-${Date.now()}`,
        invitationId,
        messageType: "user",
        messageText: userQuestion,
        createdAt: new Date().toISOString(),
      };
      setConversation((prev) => [...prev, userMessage]);

      const response = await askQuestion(invitationId, userQuestion, { accessToken });
      
      // Replace temp message and add response
      setConversation((prev) => {
        const filtered = prev.filter((m) => m.id !== userMessage.id);
        return [...filtered, response];
      });
    } catch (err) {
      setError(err instanceof Error ? err.message : "Failed to ask question");
      // Remove optimistic message on error
      setConversation((prev) => prev.filter((m) => m.id !== `temp-${Date.now()}`));
    } finally {
      setAsking(false);
    }
  }

  if (loading) {
    return (
      <Card>
        <CardContent className="py-8 text-center">
          <Loader2 className="mx-auto h-6 w-6 animate-spin text-zinc-400" />
          <p className="mt-2 text-sm text-zinc-500">Loading analysis...</p>
        </CardContent>
      </Card>
    );
  }

  return (
    <div className="space-y-6">
      <Tabs defaultValue="analysis">
        <TabsList>
          <TabsTrigger value="analysis">Analysis</TabsTrigger>
          <TabsTrigger value="qa">Ask Questions</TabsTrigger>
        </TabsList>

        <TabsContent value="analysis">
          <Card>
            <CardHeader>
              <div className="flex items-center justify-between">
                <div>
                  <CardTitle className="text-base">AI-Powered Code Analysis</CardTitle>
                  <CardDescription>
                    Automated analysis of code quality and rubric adherence using LLM
                  </CardDescription>
                </div>
                <Button
                  onClick={() => handleGenerate(!!analysis)}
                  disabled={generating}
                  variant="outline"
                  size="sm"
                >
                  {generating ? (
                    <>
                      <Loader2 className="mr-2 h-4 w-4 animate-spin" />
                      Generating...
                    </>
                  ) : analysis ? (
                    <>
                      <RefreshCw className="mr-2 h-4 w-4" />
                      Regenerate
                    </>
                  ) : (
                    "Generate Analysis"
                  )}
                </Button>
              </div>
            </CardHeader>
            <CardContent>
              {error && (
                <div className="mb-4 rounded-lg border border-red-200 bg-red-50 p-3 text-sm text-red-800">
                  {error}
                </div>
              )}

              {analysis ? (
                <div className="space-y-4">
                  {analysis.modelUsed && (
                    <p className="text-xs text-zinc-500">
                      Generated using {analysis.modelUsed} â€¢{" "}
                      {new Date(analysis.createdAt).toLocaleString()}
                    </p>
                  )}
                  <div className="prose prose-zinc max-w-none">
                    <Markdown>{analysis.analysisText}</Markdown>
                  </div>
                </div>
              ) : (
                <div className="py-8 text-center text-sm text-zinc-500">
                  <p>No analysis generated yet.</p>
                  <p className="mt-2">Click "Generate Analysis" to create an AI-powered review.</p>
                </div>
              )}
            </CardContent>
          </Card>
        </TabsContent>

        <TabsContent value="qa">
          <Card className="flex flex-col h-[600px]">
            <CardHeader>
              <CardTitle className="text-base">Ask Questions About the Codebase</CardTitle>
              <CardDescription>
                Ask follow-up questions with full context (rubric + diffs + analysis)
              </CardDescription>
            </CardHeader>
            <CardContent className="flex-1 flex flex-col">
              {error && (
                <div className="mb-4 rounded-lg border border-red-200 bg-red-50 p-3 text-sm text-red-800">
                  {error}
                </div>
              )}

              {/* Conversation messages */}
              <div className="flex-1 overflow-y-auto space-y-4 mb-4 pr-2">
                {loadingConversation ? (
                  <div className="flex justify-center py-8">
                    <Loader2 className="h-6 w-6 animate-spin text-zinc-400" />
                  </div>
                ) : conversation.length === 0 ? (
                  <div className="text-center py-8 text-sm text-zinc-500">
                    <p>No questions yet.</p>
                    <p className="mt-2">Ask a question about the codebase below.</p>
                  </div>
                ) : (
                  conversation.map((message) => (
                    <div
                      key={message.id}
                      className={`flex gap-3 ${
                        message.messageType === "user" ? "justify-end" : "justify-start"
                      }`}
                    >
                      {message.messageType === "assistant" && (
                        <div className="flex-shrink-0 w-8 h-8 rounded-full bg-blue-100 flex items-center justify-center">
                          <Bot className="h-4 w-4 text-blue-600" />
                        </div>
                      )}
                      <div
                        className={`rounded-lg p-3 max-w-[80%] ${
                          message.messageType === "user"
                            ? "bg-blue-600 text-white"
                            : "bg-zinc-100 text-zinc-900"
                        }`}
                      >
                        <div className="prose prose-sm max-w-none">
                          <Markdown
                            className={
                              message.messageType === "user" ? "text-white" : "text-zinc-900"
                            }
                          >
                            {message.messageText}
                          </Markdown>
                        </div>
                        <p className="text-xs mt-2 opacity-70">
                          {new Date(message.createdAt).toLocaleTimeString()}
                        </p>
                      </div>
                      {message.messageType === "user" && (
                        <div className="flex-shrink-0 w-8 h-8 rounded-full bg-zinc-200 flex items-center justify-center">
                          <User className="h-4 w-4 text-zinc-600" />
                        </div>
                      )}
                    </div>
                  ))
                )}
                <div ref={conversationEndRef} />
              </div>

              {/* Input area */}
              <div className="flex gap-2">
                <Textarea
                  value={question}
                  onChange={(e) => setQuestion(e.target.value)}
                  onKeyDown={(e) => {
                    if (e.key === "Enter" && !e.shiftKey) {
                      e.preventDefault();
                      handleAskQuestion();
                    }
                  }}
                  placeholder="Ask a question about the codebase, rubric adherence, code quality..."
                  className="min-h-[80px] resize-none"
                  disabled={asking || !accessToken}
                />
                <Button
                  onClick={handleAskQuestion}
                  disabled={!question.trim() || asking || !accessToken}
                  size="icon"
                  className="flex-shrink-0"
                >
                  {asking ? (
                    <Loader2 className="h-4 w-4 animate-spin" />
                  ) : (
                    <Send className="h-4 w-4" />
                  )}
                </Button>
              </div>
            </CardContent>
          </Card>
        </TabsContent>
      </Tabs>
    </div>
  );
}
```

## Environment Variables

Add to backend `.env`:

```bash
# LLM Provider Configuration
LLM_PROVIDER=openai  # or "anthropic"
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4  # or gpt-4-turbo-preview, gpt-3.5-turbo
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-opus-20240229  # or claude-3-sonnet-20240229
```

## Implementation Steps

### Phase 1: Database & Models
1. Create database migration for `review_llm_analyses` table
2. Create database migration for `review_llm_conversations` table
3. Add SQLAlchemy models in `models.py`
4. Add Pydantic schemas in `schemas.py`
5. Update `Invitation` model relationships

### Phase 2: LLM Service
1. Create `services/llm_service.py` with provider abstraction
2. Implement OpenAI provider (or Anthropic)
3. Create prompt template function
4. Add error handling and rate limiting

### Phase 3: Backend API
1. Add endpoint to get existing analysis
2. Add endpoint to generate new analysis
3. Add endpoint to get conversation history
4. Add endpoint to ask questions (with full context: rubric + diffs + history)
5. Integrate with diff fetching logic
6. **Ensure rubric is ALWAYS included in all LLM prompts**
7. Add proper error handling and validation
8. Add authentication/authorization checks

### Phase 4: Frontend
1. Add API client functions (analysis + conversation)
2. Create `LLMAnalysisTab` component with two sub-tabs:
   - Analysis tab (initial analysis generation)
   - Q&A tab (interactive conversation)
3. Integrate tab into review page
4. Add chat UI for Q&A with message history
5. Add loading and error states
6. Style with existing design system

### Phase 5: Testing & Polish
1. Test with various rubric formats
2. **Verify rubric is included in all LLM calls** (analysis + Q&A)
3. Test conversation flow (multiple questions, context preservation)
4. Test with large diffs (token limits)
5. Test error scenarios (missing rubric, API failures)
6. Add rate limiting if needed
7. Add analytics/logging

## Considerations

### Token Limits
- LLMs have token limits (e.g., GPT-4: 8k context, Claude Opus: 200k)
- Strategy: Truncate diffs to most relevant files if needed
- Prioritize modified/new files over deleted files
- Limit total diff size (e.g., max 50k characters)

### Cost Management
- Cache analyses to avoid redundant calls
- Allow regeneration only when explicitly requested
- Consider using cheaper models for initial drafts
- Monitor API usage

### Privacy & Security
- Ensure LLM provider has appropriate data handling policies
- Consider masking sensitive data in prompts
- Store analyses in secure database
- Respect user permissions

### Performance
- Generate analysis asynchronously (could use background job)
- Show loading states appropriately
- Consider streaming responses for better UX

### Prompt Engineering
- **CRITICAL: Rubric must be included as primary context in ALL prompts**
- Iterate on prompt template based on results
- Test with various rubric formats
- Consider including assessment instructions/context
- May want to include scoring features list for consistency
- For Q&A: Include conversation history to maintain context
- For Q&A: Reference the rubric when answering questions about code quality

## Future Enhancements

1. **Streaming Responses**: Stream LLM output for faster perceived performance
2. **Analysis History**: Allow multiple analyses per invitation (timeline view)
3. **Custom Prompts**: Let admins customize LLM prompts per assessment
4. **Comparison**: Compare multiple candidates' analyses side-by-side
5. **Integration**: Auto-populate feedback summary from LLM analysis
6. **Confidence Scores**: Have LLM provide confidence scores for its assessments
7. **Rubric-Specific Analysis**: Generate section-by-section breakdown matching rubric structure
8. **Question Suggestions**: Provide suggested questions based on the code and rubric
9. **Export Conversation**: Export Q&A conversation as markdown
10. **Context Awareness**: Highlight which parts of the diff/code are referenced in responses
